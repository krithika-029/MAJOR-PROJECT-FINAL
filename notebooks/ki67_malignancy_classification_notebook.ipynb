{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90695e98",
   "metadata": {},
   "source": [
    "# Ki-67 Malignancy Classification Validation\n",
    "\n",
    "This notebook classifies **ALL test images** as **MALIGNANT** or **BENIGN** based on Ki-67 cell counts and generates a confusion matrix.\n",
    "\n",
    "## How it works:\n",
    "1. **Detect cells**: Uses your trained model to detect positive/negative Ki-67 cells in each test image\n",
    "2. **Classify malignancy**: If positive cells > negative cells ‚Üí MALIGNANT, else ‚Üí BENIGN  \n",
    "3. **Compare with ground truth**: Uses H5 annotation files to get actual malignancy labels\n",
    "4. **Generate confusion matrix**: Shows how many classifications were correct\n",
    "\n",
    "## Instructions:\n",
    "1. Upload your checkpoint: `ki67-point-epoch=68-val_peak_f1_avg=0.8503.ckpt` to `/content/`\n",
    "2. Make sure `BCData.zip` is in your Google Drive (MyDrive folder)\n",
    "3. Run all cells below\n",
    "4. Get classification accuracy and confusion matrix!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74090129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and extract dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check if dataset already exists, otherwise extract\n",
    "import os\n",
    "if not os.path.exists('/content/BCData'):\n",
    "    print(\"Extracting BCData.zip from Google Drive...\")\n",
    "    !unzip -q \"/content/drive/MyDrive/BCData.zip\" -d \"/content/\"\n",
    "    print(\"‚úì Dataset extracted to /content/BCData\")\n",
    "else:\n",
    "    print(\"‚úì Dataset already exists at /content/BCData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda61a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q segmentation-models-pytorch albumentations h5py opencv-python-headless pytorch-lightning scipy scikit-image seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.ndimage import label as scipy_label\n",
    "from skimage.feature import peak_local_max\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18377e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Point Heatmap Generator (from training script)\n",
    "class ImprovedPointHeatmapGenerator:\n",
    "    def __init__(self, sigma=8.0):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def generate_heatmap(self, points, image_shape=(640, 640)):\n",
    "        if len(points) == 0:\n",
    "            return np.zeros(image_shape, dtype=np.float32)\n",
    "\n",
    "        heatmap = np.zeros(image_shape, dtype=np.float32)\n",
    "        kernel_size = int(6 * self.sigma + 1)\n",
    "        if kernel_size % 2 == 0:\n",
    "            kernel_size += 1\n",
    "\n",
    "        x = np.arange(0, kernel_size)\n",
    "        y = x[:, np.newaxis]\n",
    "        x0 = y0 = kernel_size // 2\n",
    "\n",
    "        gaussian = np.exp(-((x - x0) ** 2 + (y - y0) ** 2) / (2 * self.sigma ** 2))\n",
    "        gaussian = gaussian / gaussian.max()\n",
    "\n",
    "        for point in points:\n",
    "            x, y = int(point[0]), int(point[1])\n",
    "\n",
    "            x_min = max(0, x - kernel_size // 2)\n",
    "            x_max = min(image_shape[1], x + kernel_size // 2 + 1)\n",
    "            y_min = max(0, y - kernel_size // 2)\n",
    "            y_max = min(image_shape[0], y + kernel_size // 2 + 1)\n",
    "\n",
    "            k_x_min = max(0, kernel_size // 2 - x)\n",
    "            k_x_max = min(kernel_size, kernel_size // 2 + (image_shape[1] - x))\n",
    "            k_y_min = max(0, kernel_size // 2 - y)\n",
    "            k_y_max = min(kernel_size, kernel_size // 2 + (image_shape[0] - y))\n",
    "\n",
    "            heatmap[y_min:y_max, x_min:x_max] = np.maximum(\n",
    "                heatmap[y_min:y_max, x_min:x_max],\n",
    "                gaussian[k_y_min:k_y_max, k_x_min:k_x_max]\n",
    "            )\n",
    "\n",
    "        return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05437857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class (from training script)\n",
    "class Ki67PointDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None, heatmap_generator=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "        self.heatmap_generator = heatmap_generator or ImprovedPointHeatmapGenerator(sigma=8.0)\n",
    "\n",
    "        self.image_files = sorted([f for f in os.listdir(image_dir)\n",
    "                                   if f.endswith('.png')])\n",
    "\n",
    "        print(f\"Found {len(self.image_files)} images in {image_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def load_points_from_h5(self, h5_path):\n",
    "        try:\n",
    "            with h5py.File(h5_path, 'r') as f:\n",
    "                if 'coordinates' in f:\n",
    "                    return f['coordinates'][:]\n",
    "        except:\n",
    "            pass\n",
    "        return np.array([])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "\n",
    "        pos_h5 = os.path.join(self.annotation_dir, 'positive', f\"{base_name}.h5\")\n",
    "        neg_h5 = os.path.join(self.annotation_dir, 'negative', f\"{base_name}.h5\")\n",
    "\n",
    "        pos_points = self.load_points_from_h5(pos_h5)\n",
    "        neg_points = self.load_points_from_h5(neg_h5)\n",
    "\n",
    "        pos_heatmap = self.heatmap_generator.generate_heatmap(pos_points, image.shape[:2])\n",
    "        neg_heatmap = self.heatmap_generator.generate_heatmap(neg_points, image.shape[:2])\n",
    "\n",
    "        heatmaps = np.stack([pos_heatmap, neg_heatmap], axis=0)\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=image,\n",
    "                mask=heatmaps.transpose(1, 2, 0)\n",
    "            )\n",
    "            image = transformed['image']\n",
    "            heatmaps = transformed['mask'].permute(2, 0, 1).float()\n",
    "        else:\n",
    "            image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "            heatmaps = torch.from_numpy(heatmaps).float()\n",
    "\n",
    "        return image, heatmaps, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation (from training script)\n",
    "def get_validation_augmentation():\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class (from training script)\n",
    "class ImprovedKi67PointDetectionModel(pl.LightningModule):\n",
    "    def __init__(self, encoder_name='efficientnet-b3', learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights='imagenet',\n",
    "            in_channels=3,\n",
    "            classes=2,\n",
    "            activation=None,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malignancy Classification Functions\n",
    "def classify_malignancy_from_cells(pos_count, neg_count, threshold_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Classify image as malignant or benign based on cell counts\n",
    "\n",
    "    Args:\n",
    "        pos_count: Number of positive (Ki-67+) cells\n",
    "        neg_count: Number of negative (Ki-67-) cells\n",
    "        threshold_ratio: If pos/neg > threshold_ratio, classify as malignant\n",
    "\n",
    "    Returns:\n",
    "        'malignant' or 'benign', ki67_index\n",
    "    \"\"\"\n",
    "    total_cells = pos_count + neg_count\n",
    "    if total_cells == 0:\n",
    "        return 'benign', 0.0  # No cells = benign\n",
    "\n",
    "    ki67_index = pos_count / total_cells\n",
    "\n",
    "    # Classification logic: if more positive cells than negative, malignant\n",
    "    if pos_count > neg_count:\n",
    "        return 'malignant', ki67_index\n",
    "    else:\n",
    "        return 'benign', ki67_index\n",
    "\n",
    "def get_ground_truth_malignancy(annotation_dir, image_name):\n",
    "    \"\"\"\n",
    "    Get ground truth malignancy from H5 files\n",
    "\n",
    "    Args:\n",
    "        annotation_dir: Path to annotations directory\n",
    "        image_name: Image filename (e.g., '0.png')\n",
    "\n",
    "    Returns:\n",
    "        'malignant' or 'benign' based on ground truth cell counts\n",
    "    \"\"\"\n",
    "    base_name = os.path.splitext(image_name)[0]\n",
    "\n",
    "    pos_h5 = os.path.join(annotation_dir, 'positive', f\"{base_name}.h5\")\n",
    "    neg_h5 = os.path.join(annotation_dir, 'negative', f\"{base_name}.h5\")\n",
    "\n",
    "    pos_points = []\n",
    "    neg_points = []\n",
    "\n",
    "    # Load positive points\n",
    "    try:\n",
    "        with h5py.File(pos_h5, 'r') as f:\n",
    "            if 'coordinates' in f:\n",
    "                pos_points = f['coordinates'][:]\n",
    "    except:\n",
    "        pos_points = []\n",
    "\n",
    "    # Load negative points\n",
    "    try:\n",
    "        with h5py.File(neg_h5, 'r') as f:\n",
    "            if 'coordinates' in f:\n",
    "                neg_points = f['coordinates'][:]\n",
    "    except:\n",
    "        neg_points = []\n",
    "\n",
    "    pos_count = len(pos_points)\n",
    "    neg_count = len(neg_points)\n",
    "\n",
    "    # Ground truth classification\n",
    "    if pos_count > neg_count:\n",
    "        return 'malignant'\n",
    "    else:\n",
    "        return 'benign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a3e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Loss (from training script)\n",
    "class ImprovedHeatmapLoss(nn.Module):\n",
    "    def __init__(self, pos_weight=10.0):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss(\n",
    "            pos_weight=torch.tensor([pos_weight]),\n",
    "            reduction='mean'\n",
    "        )\n",
    "\n",
    "    def dice_loss(self, pred, target, smooth=1e-6):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        intersection = (pred * target).sum(dim=(2, 3))\n",
    "        union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "        dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "        return 1.0 - dice.mean()\n",
    "\n",
    "    def focal_loss(self, pred, target, alpha=0.25, gamma=2.0):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = alpha * (1 - pt) ** gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        bce_loss = self.bce(pred, target)\n",
    "        dice = self.dice_loss(pred, target)\n",
    "        focal = self.focal_loss(pred, target)\n",
    "        total_loss = 0.4 * bce_loss + 0.4 * dice + 0.2 * focal\n",
    "        return total_loss, {\n",
    "            'bce': bce_loss.item(),\n",
    "            'dice': dice.item(),\n",
    "            'focal': focal.item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d631b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class (from training script)\n",
    "class ImprovedKi67PointDetectionModel(pl.LightningModule):\n",
    "    def __init__(self, encoder_name='efficientnet-b3', learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights='imagenet',\n",
    "            in_channels=3,\n",
    "            classes=2,\n",
    "            activation=None,\n",
    "        )\n",
    "\n",
    "        # Add the criterion to match the saved checkpoint\n",
    "        self.criterion = ImprovedHeatmapLoss(pos_weight=10.0)\n",
    "        self.validation_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195138b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main validation function\n",
    "def validate_malignancy_classification(checkpoint_path, data_path='/content/BCData', batch_size=8):\n",
    "    \"\"\"\n",
    "    Validate malignancy classification accuracy on test set\n",
    "    Returns classification metrics and confusion matrix\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üî¨ MALIGNANCY CLASSIFICATION VALIDATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Checkpoint: {checkpoint_path}\")\n",
    "    print(f\"Dataset: {data_path}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # Check if files exist\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"‚ùå Checkpoint not found: {checkpoint_path}\")\n",
    "        return None\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"‚ùå Dataset not found: {data_path}\")\n",
    "        return None\n",
    "\n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    try:\n",
    "        model = ImprovedKi67PointDetectionModel.load_from_checkpoint(checkpoint_path)\n",
    "        print(\"‚úì Model loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load model: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"‚úì Using device: {device}\")\n",
    "\n",
    "    # Create test dataset\n",
    "    heatmap_gen = ImprovedPointHeatmapGenerator(sigma=8.0)\n",
    "\n",
    "    try:\n",
    "        test_dataset = Ki67PointDataset(\n",
    "            image_dir=os.path.join(data_path, 'images/test'),\n",
    "            annotation_dir=os.path.join(data_path, 'annotations/test'),\n",
    "            transform=get_validation_augmentation(),\n",
    "            heatmap_generator=heatmap_gen\n",
    "        )\n",
    "        print(f\"‚úì Test dataset created: {len(test_dataset)} images\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Create dataloader\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Classification results\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    ki67_indices = []\n",
    "    cell_counts = []\n",
    "\n",
    "    print(\"\\nRunning malignancy classification...\")\n",
    "    print(\"Classifying each image as MALIGNANT or BENIGN...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Processing images\"):\n",
    "            images, heatmaps, names = batch\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            pred_heatmaps = torch.sigmoid(outputs)\n",
    "\n",
    "            # Process each image in batch\n",
    "            for i in range(len(names)):\n",
    "                pred_pos = pred_heatmaps[i, 0].float().numpy()\n",
    "                pred_neg = pred_heatmaps[i, 1].float().numpy()\n",
    "\n",
    "                # Detect peaks (cell locations)\n",
    "                pred_peaks_pos = peak_local_max(pred_pos, threshold_abs=0.3, min_distance=10)\n",
    "                pred_peaks_neg = peak_local_max(pred_neg, threshold_abs=0.3, min_distance=10)\n",
    "\n",
    "                pos_count = len(pred_peaks_pos)\n",
    "                neg_count = len(pred_peaks_neg)\n",
    "\n",
    "                # Classify malignancy\n",
    "                prediction, ki67_index = classify_malignancy_from_cells(pos_count, neg_count)\n",
    "\n",
    "                # Get ground truth\n",
    "                ground_truth = get_ground_truth_malignancy(\n",
    "                    os.path.join(data_path, 'annotations/test'),\n",
    "                    names[i]\n",
    "                )\n",
    "\n",
    "                # Store results\n",
    "                predictions.append(prediction)\n",
    "                ground_truths.append(ground_truth)\n",
    "                ki67_indices.append(ki67_index)\n",
    "                cell_counts.append((pos_count, neg_count))\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    ground_truths = np.array(ground_truths)\n",
    "    ki67_indices = np.array(ki67_indices)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(ground_truths, predictions)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(ground_truths, predictions, labels=['benign', 'malignant'])\n",
    "\n",
    "    # Classification report\n",
    "    report = classification_report(ground_truths, predictions,\n",
    "                                 labels=['benign', 'malignant'],\n",
    "                                 target_names=['Benign', 'Malignant'],\n",
    "                                 output_dict=True)\n",
    "\n",
    "    # Results\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report,\n",
    "        'predictions': predictions,\n",
    "        'ground_truths': ground_truths,\n",
    "        'ki67_indices': ki67_indices,\n",
    "        'cell_counts': cell_counts,\n",
    "        'num_images': len(predictions)\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"üìä MALIGNANCY CLASSIFICATION RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "    print(f\"Total Images: {len(predictions)}\")\n",
    "    print()\n",
    "\n",
    "    print(\"CONFUSION MATRIX:\")\n",
    "    print(\"                 Predicted\")\n",
    "    print(\"                 Benign    Malignant\")\n",
    "    print(f\"Actual  Benign    {cm[0,0]:<8}  {cm[0,1]:<8}\")\n",
    "    print(f\"        Malignant {cm[1,0]:<8}  {cm[1,1]:<8}\")\n",
    "    print()\n",
    "\n",
    "    print(\"CLASSIFICATION REPORT:\")\n",
    "    print(f\"Benign    - Precision: {report['Benign']['precision']:.3f}, Recall: {report['Benign']['recall']:.3f}, F1: {report['Benign']['f1-score']:.3f}\")\n",
    "    print(f\"Malignant - Precision: {report['Malignant']['precision']:.3f}, Recall: {report['Malignant']['recall']:.3f}, F1: {report['Malignant']['f1-score']:.3f}\")\n",
    "    print()\n",
    "\n",
    "    # Statistics\n",
    "    malignant_pred = np.sum(predictions == 'malignant')\n",
    "    benign_pred = np.sum(predictions == 'benign')\n",
    "    malignant_gt = np.sum(ground_truths == 'malignant')\n",
    "    benign_gt = np.sum(ground_truths == 'benign')\n",
    "\n",
    "    print(\"STATISTICS:\")\n",
    "    print(f\"Ground Truth - Malignant: {malignant_gt}, Benign: {benign_gt}\")\n",
    "    print(f\"Predictions  - Malignant: {malignant_pred}, Benign: {benign_pred}\")\n",
    "    print()\n",
    "\n",
    "    # Ki-67 Index statistics\n",
    "    malignant_ki67 = ki67_indices[ground_truths == 'malignant']\n",
    "    benign_ki67 = ki67_indices[ground_truths == 'benign']\n",
    "\n",
    "    print(\"KI-67 INDEX STATISTICS:\")\n",
    "    print(\".3f\")\n",
    "    print(\".3f\")\n",
    "    print()\n",
    "\n",
    "    # Accuracy interpretation\n",
    "    print(\"ACCURACY INTERPRETATION:\")\n",
    "    if accuracy >= 0.90:\n",
    "        print(\"üéâ EXCELLENT: 90%+ classification accuracy!\")\n",
    "    elif accuracy >= 0.80:\n",
    "        print(\"‚úÖ VERY GOOD: 80%+ classification accuracy\")\n",
    "    elif accuracy >= 0.70:\n",
    "        print(\"‚ö†Ô∏è  GOOD: 70%+ classification accuracy\")\n",
    "    elif accuracy >= 0.60:\n",
    "        print(\"ü§î FAIR: 60%+ classification accuracy - needs improvement\")\n",
    "    else:\n",
    "        print(\"‚ùå POOR: <60% classification accuracy - retrain needed\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix visualization\n",
    "def plot_confusion_matrix(results):\n",
    "    \"\"\"Plot confusion matrix heatmap\"\"\"\n",
    "    cm = results['confusion_matrix']\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Benign', 'Malignant'],\n",
    "                yticklabels=['Benign', 'Malignant'])\n",
    "    plt.title('Malignancy Classification Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('Actual', fontsize=14)\n",
    "    plt.xlabel('Predicted', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347f86d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "print(\"üî¨ Ki-67 Malignancy Classification Validation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configuration\n",
    "CHECKPOINT_PATH = '/content/ki67-point-epoch=68-val_peak_f1_avg=0.8503.ckpt'\n",
    "DATA_PATH = '/content/BCData'\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Run validation\n",
    "results = validate_malignancy_classification(\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    data_path=DATA_PATH,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "if results:\n",
    "    print(\"‚úÖ Classification validation completed successfully!\")\n",
    "    print(f\"üìä Key Results:\")\n",
    "    print(\".1f\")\n",
    "    print(f\"   Total Images: {results['num_images']}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    print(\"\\nüìà Generating confusion matrix visualization...\")\n",
    "    try:\n",
    "        plot_confusion_matrix(results)\n",
    "        print(\"‚úì Confusion matrix plotted\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Visualization failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Classification validation failed - check paths and files\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üéâ Script Complete!\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
